#!/usr/bin/env python
#   Parallel uploads to Amazon AWS S3
#   Copyright (C) 2011  Tom Payne
#
#   This program is free software: you can redistribute it and/or modify
#   it under the terms of the GNU General Public License as published by
#   the Free Software Foundation, either version 3 of the License, or
#   (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program.  If not, see <http://www.gnu.org/licenses/>.

import logging
from multiprocessing import JoinableQueue, Process, current_process
from optparse import OptionParser
import os.path
import sys
import time

from boto.s3.connection import S3Connection


def walker(put_queue, sources, options):
    logger = logging.getLogger('%s[walker-%d]' % (os.path.basename(sys.argv[0]), current_process().pid))
    logger.debug('starting')
    limit = options.limit
    for source in sources:
        if os.path.isdir(source):
            for dirpath, dirnames, filenames in os.walk(source):
                for filename in filenames:
                    path = os.path.join(dirpath, filename)
                    if not os.path.isfile(path):
                        continue
                    if limit == 0:
                        return
                    limit -= 1
                    key_name = os.path.normpath(os.path.join(options.destination, path))
                    put_queue.put([path, key_name])
        elif os.path.isfile(source):
            if limit == 0:
                return
            limit -= 1
            key_name = os.path.normpath(os.path.join(options.destination, source))
            put_queue.put([source, key_name])
    logger.debug('terminating')


def putter(put_queue, stat_queue, options):
    logger = logging.getLogger('%s[putter-%d]' % (os.path.basename(sys.argv[0]), current_process().pid))
    logger.debug('starting')
    if options.mode != 'offline':
        connection = S3Connection(is_secure=options.secure)
        bucket = connection.get_bucket(options.bucket)
    while True:
        args = put_queue.get()
        if args is None:
            put_queue.task_done()
            break
        filename, key_name = args
        if options.mode == 'add':
            key = bucket.get_key(key_name)
            if key is None:
                with open(filename) as fp:
                    key = bucket.new_key(key_name)
                    if not options.dry_run:
                        key.set_contents_from_file(fp)
                    logger.info('%s -> %s' % (filename, key.name))
                    stat_queue.put(os.fstat(fp.fileno()))
            else:
                logger.info('skipping %s -> %s (already exists)' % (filename, key_name))
        elif options.mode == 'stupid':
            with open(filename) as fp:
                key = bucket.new_key(key_name)
                if not options.dry_run:
                    key.set_contents_from_file(fp)
                logger.info('%s -> %s' % (filename, key.name))
                stat_queue.put(os.fstat(fp.fileno()))
        elif options.mode == 'offline':
            logger.info('%s -> %s' % (filename, key_name))
            stat_queue.put(os.stat(filename))
        elif options.mode == 'update':
            key = bucket.get_key(key_name)
            if key is None:
                with open(filename) as fp:
                    key = bucket.new_key(key_name)
                    if not options.dry_run:
                        key.set_contents_from_file(fp)
                    logger.info('%s -> %s' % (filename, key.name))
                    stat_queue.put(os.fstat(fp.fileno()))
            else:
                with open(filename) as fp:
                    md5 = key.compute_md5(fp)
                    if key.etag == '"%s"' % md5[0]:
                        logger.info('skipping %s -> %s (content unchanged)' % (filename, key.name))
                    else:
                        if not options.dry_run:
                            key.set_contents_from_file(fp)
                        logger.info('%s -> %s' % (filename, key.name))
                        stat_queue.put(os.fstat(fp.fileno()))
        else:
            assert False
        put_queue.task_done()
    logger.debug('terminating')


def statter(stat_queue, start, options):
    logger = logging.getLogger('%s[statter-%d]' % (os.path.basename(sys.argv[0]), current_process().pid))
    logger.debug('starting')
    count, total_size = 0, 0
    while True:
        stat = stat_queue.get()
        if stat is None:
            stat_queue.task_done()
            break
        count += 1
        total_size += stat.st_size
        stat_queue.task_done()
    duration = time.time() - start
    logger.info('put %d bytes in %d files in %.1f seconds (%d bytes/s, %.1f files/s)' % (total_size, count, duration, total_size / duration, count / duration))
    logger.debug('terminating')


def main(argv):
    parser = OptionParser()
    parser.add_option('--bucket', '-b', metavar='BUCKET')
    parser.add_option('--destination', '-d', metavar='DESTINATION')
    parser.add_option('--dry-run', action='store_true')
    parser.add_option('--insecure', action='store_false', dest='secure')
    parser.add_option('--limit', default=-1, metavar='N', type=int)
    parser.add_option('--mode', choices=('add', 'offline', 'stupid', 'update'), default='update', metavar='MODE')
    parser.add_option('--processes', '-p', default=8, metavar='PROCESSES', type=int)
    parser.add_option('--quiet', '-q', action='count', default=0)
    parser.add_option('--secure', action='store_true', default=True, dest='secure')
    parser.add_option('--verbose', '-v', action='count', default=0)
    options, args = parser.parse_args(argv[1:])
    logging.basicConfig(level=logging.INFO + 10 * (options.quiet - options.verbose))
    logger = logging.getLogger(os.path.basename(sys.argv[0]))
    if len(args) < 1:
        logger.error('missing source operand')
        return 1
    if not options.bucket:
        logger.error('missing bucket')
        return 1
    if not options.destination:
        logger.error('missing destination')
        return 1
    if options.mode != 'offline':
        # Test connection to S3
        connection = S3Connection(is_secure=options.secure)
        bucket = connection.get_bucket(options.bucket)
        del bucket
        del connection
    start = time.time()
    put_queue = JoinableQueue()
    stat_queue = JoinableQueue()
    walker_process = Process(target=walker, args=(put_queue, args, options))
    walker_process.start()
    logger.debug('started walker-%d' % walker_process.pid)
    putter_processes = [Process(target=putter, args=(put_queue, stat_queue, options)) for i in xrange(options.processes)]
    for putter_process in putter_processes:
        putter_process.start()
        logger.debug('started putter-%d' % putter_process.pid)
    statter_process = Process(target=statter, args=(stat_queue, start, options))
    statter_process.start()
    logger.debug('started statter-%d' % statter_process.pid)
    logger.debug('joining walker-%d' % walker_process.pid)
    walker_process.join()
    logger.debug('shutting down put_queue')
    for putter_process in putter_processes:
        put_queue.put(None)
    put_queue.close()
    for putter_process in putter_processes:
        logger.debug('joining putter-%d' % putter_process.pid)
        putter_process.join()
    logger.debug('shutting down stat_queue')
    stat_queue.put(None)
    stat_queue.close()
    logger.debug('joining statter-%d' % walker_process.pid)
    statter_process.join()
    logger.debug('joining put_queue')
    put_queue.join_thread()
    logger.debug('joining stat_queue')
    stat_queue.join_thread()


if __name__ == '__main__':
    sys.exit(main(sys.argv))
