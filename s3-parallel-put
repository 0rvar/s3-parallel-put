#!/usr/bin/env python
#   Parallel uploads to Amazon AWS S3
#   Copyright (C) 2011  Tom Payne
#
#   This program is free software: you can redistribute it and/or modify
#   it under the terms of the GNU General Public License as published by
#   the Free Software Foundation, either version 3 of the License, or
#   (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program.  If not, see <http://www.gnu.org/licenses/>.

import hashlib
import logging
from multiprocessing import JoinableQueue, Process, current_process
from optparse import OptionParser
import os.path
import sys
import tarfile
import time

from boto.s3.connection import S3Connection


class Done(Exception):
    pass


class Walker(object):

    def __init__(self, put_queue, sources, options):
        self.put_queue = put_queue
        self.sources = sources
        self.options = options
        self.logger = logging.getLogger('%s[walker-%d]' % (os.path.basename(sys.argv[0]), current_process().pid))
        self.logger.debug('starting')
        limit = self.options.limit
        try:
            for source in sources:
                for kwargs in self.walk(source):
                    if limit == 0:
                        raise Done
                    limit -= 1
                    put_queue.put(kwargs)
        except Done:
            pass
        self.logger.debug('terminating')

    def walk(self, source):
        raise NotImplementedError


class TarWalker(Walker):

    def walk(self, source):
        try:
            tf = tarfile.open(source, 'r:')
            for tarinfo in tf:
                if tarinfo.isfile():
                    path = tarinfo.name
                    key_name = os.path.normpath(os.path.join(self.options.destination, path))
                    filename = source
                    offset = tarinfo.offset_data
                    size = tarinfo.size
                    yield(dict(filename=source, key_name=key_name, offset=offset, path=path, size=size))
        except tarfile.ReadError:
            tf = tarfile.open(source)
            for tarinfo in tf:
                if tarinfo.isfile():
                    path = tarinfo.name
                    key_name = os.path.normpath(os.path.join(self.options.destination, path))
                    content = tf.extractfile(tarinfo).read()
                    yield dict(content=content, key_name=key_name, path=path)


class TreeWalker(Walker):

    def walk(self, source):
        if os.path.isdir(source):
            for dirpath, dirnames, filenames in os.walk(source):
                for filename in filenames:
                    path = os.path.join(dirpath, filename)
                    if not os.path.isfile(path):
                        continue
                    key_name = os.path.normpath(os.path.join(self.options.destination, path))
                    yield dict(key_name=key_name, path=path)
        elif os.path.isfile(source):
            key_name = os.path.normpath(os.path.join(self.options.destination, source))
            yield dict(key_name=key_name, path=path)


class SimplePutter(object):

    def __init__(self, put_queue, stat_queue, options):
        self.logger = logging.getLogger('%s[putter-%d]' % (os.path.basename(sys.argv[0]), current_process().pid))
        self.logger.debug('starting')
        self.put_queue = put_queue
        self.stat_queue = stat_queue
        self.options = options
        if self.options.mode == 'offline':
            self.bucket = None
        else:
            connection = S3Connection(is_secure=self.options.secure)
            self.bucket = connection.get_bucket(self.options.bucket)
        while True:
            kwargs = self.put_queue.get()
            if kwargs is None:
                self.put_queue.task_done()
                break
            self.handle(**kwargs)
            self.put_queue.task_done()
        self.logger.debug('terminating')

    def handle(self, key_name=None, **kwargs):
        key = self.bucket.new_key(key_name)
        self.put(key, **kwargs)

    def put(self, key, content=None, filename=None, fp=None, md5=None, offset=None, path=None, size=None):
        if content:
            if not self.options.dry_run:
                key.set_contents_from_string(content, md5=md5)
            size = len(content)
        elif fp:
            if not self.options.dry_run:
                key.set_contents_from_file(fp, md5=md5)
            size = os.fstat(fp.fileno()).st_size
        elif filename:
            if not self.options.dry_run:
                with open(filename) as fp:
                    fp.seek(offset)
                    content = fp.read(size)
                    try:
                        key.set_contents_from_string(m[delta:], md5=md5)
                    finally:
                        m.close()
        elif path:
            if self.options.dry_run:
                size = os.stat(path).st_size
            else:
                with open(path) as fp:
                    key.set_contents_from_file(fp, md5=md5)
                    size = os.fstat(fp.fileno()).st_size
        else:
            assert False
        self.logger.info('%s -> %s' % (path, key.name))
        self.stat_queue.put(dict(size=size))


class AddPutter(SimplePutter):

    def handle(self, key_name=None, path=None, **kwargs):
        key = self.bucket.get_key(key_name)
        if key is None:
            key = self.bucket.new_key(key_name)
            self.put(key, path=path, **kwargs)
        else:
            self.logger.info('skipping %r -> %r (key exists)' % (path, key_name))


class OfflinePutter(SimplePutter):

    def handle(self, content=None, key_name=None, path=None, size=None, **kwargs):
        if size is not None:
            pass
        elif content:
            size = len(content)
        else:
            size = os.stat(path).st_size
        self.logger.info('%s -> %s' % (path, key_name))
        self.stat_queue.put(dict(size=size))


class UpdatePutter(SimplePutter):

    def handle(self, content=None, filename=None, key_name=None, offset=None, path=None, size=None, **kwargs):
        key = self.bucket.get_key(key_name)
        if key is None:
            key = self.bucket.new_key(key_name)
            self.put(key, content=content, path=path, **kwargs)
        elif content:
            md5 = hashlib.md5(content).hexdigest()
            if key.etag == '"%s"' % md5:
                self.logger.info('skipping %r -> %r (value unchanged)' % (path, key_name))
            else:
                self.put(key, content=content, md5=key.get_md5_from_hexdigest(md5), path=path, **kwargs)
        elif filename:
            with open(filename) as fp:
                fp.seek(offset)
                content = fp.read(size)
                md5 = hashlib.md5(content).hexdigest()
                if key.etag == '"%s"' % md5:
                    self.logger.info('skipping %r -> %r (value unchanged)' % (path, key_name))
                else:
                    self.put(key, content=content, md5=key.get_md5_from_hexdigest(md5), path=path, **kwargs)
        elif path:
            with open(path) as fp:
                md5 = key.compute_md5(fp)
                if key.etag == '"%s"' % md5[0]:
                    self.logger.info('skipping %s -> %s (value unchanged)' % (path, key.name))
                else:
                    self.put(key, fp=fp, md5=md5, path=path)
        else:
            assert False


def statter(stat_queue, start, options):
    logger = logging.getLogger('%s[statter-%d]' % (os.path.basename(sys.argv[0]), current_process().pid))
    logger.debug('starting')
    count, total_size = 0, 0
    while True:
        kwargs = stat_queue.get()
        if kwargs is None:
            stat_queue.task_done()
            break
        count += 1
        total_size += kwargs.get('size', 0)
        stat_queue.task_done()
    duration = time.time() - start
    logger.info('put %d bytes in %d files in %.1f seconds (%d bytes/s, %.1f files/s)' % (total_size, count, duration, total_size / duration, count / duration))
    logger.debug('terminating')


def main(argv):
    parser = OptionParser()
    parser.add_option('--bucket', '-b', metavar='BUCKET')
    parser.add_option('--destination', '-d', metavar='DESTINATION')
    parser.add_option('--dry-run', action='store_true')
    parser.add_option('--insecure', action='store_false', dest='secure')
    parser.add_option('--limit', default=-1, metavar='N', type=int)
    parser.add_option('--mode', choices=('add', 'offline', 'stupid', 'update'), default='update', metavar='MODE')
    parser.add_option('--processes', '-p', default=8, metavar='PROCESSES', type=int)
    parser.add_option('--quiet', '-q', action='count', default=0)
    parser.add_option('--secure', action='store_true', default=True, dest='secure')
    parser.add_option('--tar', action='store_true')
    parser.add_option('--verbose', '-v', action='count', default=0)
    options, args = parser.parse_args(argv[1:])
    logging.basicConfig(level=logging.INFO + 10 * (options.quiet - options.verbose))
    logger = logging.getLogger(os.path.basename(sys.argv[0]))
    if len(args) < 1:
        logger.error('missing source operand')
        return 1
    if not options.bucket:
        logger.error('missing bucket')
        return 1
    if not options.destination:
        logger.error('missing destination')
        return 1
    if options.mode != 'offline':
        # Test connection to S3
        connection = S3Connection(is_secure=options.secure)
        bucket = connection.get_bucket(options.bucket)
        del bucket
        del connection
    start = time.time()
    put_queue = JoinableQueue()
    stat_queue = JoinableQueue()
    walker = TarWalker if options.tar else TreeWalker
    walker_process = Process(target=walker, args=(put_queue, args, options))
    walker_process.start()
    logger.debug('started walker-%d' % walker_process.pid)
    putters = dict(add=AddPutter, offline=OfflinePutter, stupid=SimplePutter, update=UpdatePutter)
    putter_processes = [Process(target=putters[options.mode], args=(put_queue, stat_queue, options)) for i in xrange(options.processes)]
    for putter_process in putter_processes:
        putter_process.start()
        logger.debug('started putter-%d' % putter_process.pid)
    statter_process = Process(target=statter, args=(stat_queue, start, options))
    statter_process.start()
    logger.debug('started statter-%d' % statter_process.pid)
    logger.debug('joining walker-%d' % walker_process.pid)
    walker_process.join()
    logger.debug('shutting down put_queue')
    for putter_process in putter_processes:
        put_queue.put(None)
    put_queue.close()
    for putter_process in putter_processes:
        logger.debug('joining putter-%d' % putter_process.pid)
        putter_process.join()
    logger.debug('shutting down stat_queue')
    stat_queue.put(None)
    stat_queue.close()
    logger.debug('joining statter-%d' % walker_process.pid)
    statter_process.join()
    logger.debug('joining put_queue')
    put_queue.join_thread()
    logger.debug('joining stat_queue')
    stat_queue.join_thread()


if __name__ == '__main__':
    sys.exit(main(sys.argv))
