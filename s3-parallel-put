#!/usr/bin/env python
#   Parallel uploads to Amazon AWS S3
#   Copyright (C) 2011  Tom Payne
#
#   This program is free software: you can redistribute it and/or modify
#   it under the terms of the GNU General Public License as published by
#   the Free Software Foundation, either version 3 of the License, or
#   (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program.  If not, see <http://www.gnu.org/licenses/>.

import logging
from multiprocessing import JoinableQueue, Process, current_process
from optparse import OptionParser
import os.path
import sys

from boto.s3.connection import S3Connection


def walker(put_queue, sources, destination, options):
    logger = logging.getLogger('%s[walker-%d]' % (os.path.basename(sys.argv[0]), current_process().pid))
    logger.debug('starting')
    limit = options.limit
    for source in sources:
        for dirpath, dirnames, filenames in os.walk(source):
            for filename in filenames:
                path = os.path.join(dirpath, filename)
                if not os.path.isfile(path):
                    continue
                if limit == 0:
                    return
                limit -= 1
                key_name = os.path.normpath(os.path.join(destination, path))
                put_queue.put([path, key_name])
    logger.debug('terminating')


def putter(put_queue, stat_queue, options):
    logger = logging.getLogger('%s[putter-%d]' % (os.path.basename(sys.argv[0]), current_process().pid))
    logger.debug('starting')
    if not options.dry_run:
        connection = S3Connection(is_secure=options.secure)
        bucket = connection.get_bucket(options.bucket)
    while True:
        args = put_queue.get()
        if args is None:
            put_queue.task_done()
            break
        filename, key_name = args
        if options.dry_run:
            stat = os.stat(filename)
        else:
            key = bucket.new_key(key_name)
            with open(filename) as fp:
                stat = os.fstat(fp.fileno)
                def cb(bytes_so_far, total_bytes):
                    logger.debug('%s -> %s %d/%d (%d%%)' % (filename, key_name, bytes_so_far, total_bytes, 100 * bytes_so_far / total_bytes))
                key.set_contents_from_file(fp, cb=cb)
        stat_queue.put(stat)
        logger.info('%s -> %s' % (filename, key_name))
        put_queue.task_done()
    logger.debug('terminating')


def statter(stat_queue, options):
    logger = logging.getLogger('%s[statter-%d]' % (os.path.basename(sys.argv[0]), current_process().pid))
    logger.debug('starting')
    count, total_size = 0, 0
    while True:
        stat = stat_queue.get()
        if stat is None:
            stat_queue.task_done()
            break
        count += 1
        total_size += stat.st_size
        stat_queue.task_done()
    logger.info('put %d bytes in %d files' % (total_size, count))
    logger.debug('terminating')



def main(argv):
    parser = OptionParser()
    parser.add_option('--bucket', '-b', metavar='BUCKET')
    parser.add_option('--dry-run', action='store_true')
    parser.add_option('--insecure', action='store_false', dest='secure')
    parser.add_option('--limit', default=-1, metavar='N', type=int)
    parser.add_option('--processes', '-p', default=4, metavar='PROCESSES', type=int)
    parser.add_option('--quiet', '-q', action='count', default=0)
    parser.add_option('--secure', action='store_true', default=True, dest='secure')
    parser.add_option('--verbose', '-v', action='count', default=0)
    options, args = parser.parse_args(argv[1:])
    logging.basicConfig(level=logging.INFO + 10 * (options.quiet - options.verbose))
    logger = logging.getLogger(os.path.basename(sys.argv[0]))
    if len(args) < 1:
        logger.error('missing source operand')
        return 1
    if len(args) < 2:
        logger.error('missing destination operand after %r' % args[0])
        return 1
    if not options.bucket:
        logger.error('missing bucket')
        return 1
    if not options.dry_run:
        # Test connection to S3
        connection = S3Connection(is_secure=options.secure)
        bucket = connection.get_bucket(options.bucket)
        del bucket
        del connection
    sources, destination = args[:-1], args[-1]
    put_queue = JoinableQueue()
    stat_queue = JoinableQueue()
    walker_process = Process(target=walker, args=(put_queue, sources, destination, options))
    walker_process.start()
    logger.debug('started walker-%d' % walker_process.pid)
    putter_processes = [Process(target=putter, args=(put_queue, stat_queue, options)) for i in xrange(options.processes)]
    for putter_process in putter_processes:
        putter_process.start()
        logger.debug('started putter-%d' % putter_process.pid)
    statter_process = Process(target=statter, args=(stat_queue, options))
    statter_process.start()
    logger.debug('started statter-%d' % statter_process.pid)
    logger.debug('joining walker-%d' % walker_process.pid)
    walker_process.join()
    logger.debug('shutting down put_queue')
    for putter_process in putter_processes:
        put_queue.put(None)
    put_queue.close()
    for putter_process in putter_processes:
        logger.debug('joining putter-%d' % putter_process.pid)
        putter_process.join()
    logger.debug('shutting down stat_queue')
    stat_queue.put(None)
    stat_queue.close()
    logger.debug('joining statter-%d' % walker_process.pid)
    statter_process.join()
    logger.debug('joining put_queue')
    put_queue.join_thread()
    logger.debug('joining stat_queue')
    stat_queue.join_thread()


if __name__ == '__main__':
    sys.exit(main(sys.argv))
